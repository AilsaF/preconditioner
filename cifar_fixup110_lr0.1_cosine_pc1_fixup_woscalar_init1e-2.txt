['fixup_resnet110', 'fixup_resnet1202', 'fixup_resnet20', 'fixup_resnet32', 'fixup_resnet44', 'fixup_resnet56']
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
FixupResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (1): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (2): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (3): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (4): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (5): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (6): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (7): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (8): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (9): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (10): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (11): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (12): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (13): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (14): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (15): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (16): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (17): FixupBasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
  (layer2): Sequential(
    (0): FixupBasicBlock(
      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (downsample): AvgPool2d(kernel_size=1, stride=2, padding=0)
    )
    (1): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (2): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (3): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (4): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (5): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (6): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (7): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (8): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (9): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (10): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (11): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (12): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (13): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (14): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (15): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (16): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (17): FixupBasicBlock(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
  (layer3): Sequential(
    (0): FixupBasicBlock(
      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (downsample): AvgPool2d(kernel_size=1, stride=2, padding=0)
    )
    (1): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (2): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (3): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (4): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (5): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (6): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (7): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (8): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (9): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (10): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (11): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (12): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (13): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (14): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (15): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (16): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
    (17): FixupBasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=64, out_features=10, bias=True)
)
Files already downloaded and verified
current lr 1.00000e-02
Epoch: [0][0/391]	Time 11.828 (11.828)	Data 0.135 (0.135)	Loss 2.3026 (2.3026)	Prec@1 9.375 (9.375)
Epoch: [0][50/391]	Time 0.118 (0.348)	Data 0.001 (0.004)	Loss 2.2038 (2.2062)	Prec@1 17.969 (15.717)
Epoch: [0][100/391]	Time 0.123 (0.235)	Data 0.001 (0.003)	Loss 2.0292 (2.1474)	Prec@1 29.688 (18.557)
Epoch: [0][150/391]	Time 0.115 (0.198)	Data 0.001 (0.002)	Loss 1.9548 (2.0847)	Prec@1 22.656 (21.223)
Epoch: [0][200/391]	Time 0.127 (0.179)	Data 0.001 (0.002)	Loss 1.8146 (2.0428)	Prec@1 32.031 (22.715)
Epoch: [0][250/391]	Time 0.127 (0.167)	Data 0.001 (0.002)	Loss 1.7620 (2.0001)	Prec@1 30.469 (24.072)
Epoch: [0][300/391]	Time 0.130 (0.160)	Data 0.002 (0.002)	Loss 1.6962 (1.9625)	Prec@1 32.031 (25.638)
Epoch: [0][350/391]	Time 0.127 (0.155)	Data 0.001 (0.002)	Loss 1.6654 (1.9276)	Prec@1 41.406 (27.110)
Test: [0/79]	Time 0.155 (0.155)	Loss 1.5618 (1.5618)	Prec@1 41.406 (41.406)
Test: [50/79]	Time 0.043 (0.045)	Loss 1.6224 (1.5587)	Prec@1 35.156 (43.076)
 * Prec@1 42.630
current lr 9.99938e-03
Epoch: [1][0/391]	Time 0.299 (0.299)	Data 0.150 (0.150)	Loss 1.5992 (1.5992)	Prec@1 41.406 (41.406)
Epoch: [1][50/391]	Time 0.119 (0.127)	Data 0.001 (0.004)	Loss 1.5076 (1.6014)	Prec@1 41.406 (40.839)
Epoch: [1][100/391]	Time 0.128 (0.125)	Data 0.001 (0.003)	Loss 1.3800 (1.5760)	Prec@1 46.875 (42.048)
Epoch: [1][150/391]	Time 0.118 (0.124)	Data 0.001 (0.002)	Loss 1.4800 (1.5491)	Prec@1 45.312 (43.362)
Epoch: [1][200/391]	Time 0.124 (0.123)	Data 0.002 (0.002)	Loss 1.3406 (1.5238)	Prec@1 56.250 (44.220)
Epoch: [1][250/391]	Time 0.114 (0.122)	Data 0.001 (0.002)	Loss 1.4748 (1.5041)	Prec@1 47.656 (44.951)
Epoch: [1][300/391]	Time 0.117 (0.122)	Data 0.001 (0.002)	Loss 1.3669 (1.4892)	Prec@1 53.125 (45.660)
Epoch: [1][350/391]	Time 0.124 (0.122)	Data 0.002 (0.002)	Loss 1.2824 (1.4692)	Prec@1 53.125 (46.394)
Test: [0/79]	Time 0.172 (0.172)	Loss 1.3208 (1.3208)	Prec@1 51.562 (51.562)
Test: [50/79]	Time 0.042 (0.046)	Loss 1.4508 (1.3741)	Prec@1 46.875 (50.766)
 * Prec@1 50.200
current lr 9.99753e-03
Epoch: [2][0/391]	Time 0.302 (0.302)	Data 0.164 (0.164)	Loss 1.2652 (1.2652)	Prec@1 48.438 (48.438)
Epoch: [2][50/391]	Time 0.136 (0.126)	Data 0.002 (0.005)	Loss 1.3377 (1.3292)	Prec@1 48.438 (52.359)
Epoch: [2][100/391]	Time 0.134 (0.125)	Data 0.002 (0.003)	Loss 1.2895 (1.3407)	Prec@1 60.156 (52.181)
Epoch: [2][150/391]	Time 0.124 (0.125)	Data 0.002 (0.003)	Loss 1.3142 (1.3343)	Prec@1 55.469 (52.639)
Epoch: [2][200/391]	Time 0.115 (0.124)	Data 0.001 (0.002)	Loss 1.2146 (1.3196)	Prec@1 59.375 (53.113)
Epoch: [2][250/391]	Time 0.117 (0.124)	Data 0.001 (0.002)	Loss 1.3507 (1.3058)	Prec@1 49.219 (53.604)
Epoch: [2][300/391]	Time 0.119 (0.124)	Data 0.002 (0.002)	Loss 1.1978 (1.2956)	Prec@1 61.719 (54.028)
Epoch: [2][350/391]	Time 0.118 (0.124)	Data 0.001 (0.002)	Loss 1.1596 (1.2880)	Prec@1 61.719 (54.400)
Test: [0/79]	Time 0.162 (0.162)	Loss 1.1323 (1.1323)	Prec@1 64.844 (64.844)
Test: [50/79]	Time 0.043 (0.045)	Loss 1.2628 (1.1659)	Prec@1 52.344 (58.624)
 * Prec@1 58.720
current lr 9.99445e-03
Epoch: [3][0/391]	Time 0.298 (0.298)	Data 0.156 (0.156)	Loss 1.1300 (1.1300)	Prec@1 53.906 (53.906)
Epoch: [3][50/391]	Time 0.131 (0.125)	Data 0.001 (0.004)	Loss 1.3343 (1.1707)	Prec@1 53.906 (58.180)
Epoch: [3][100/391]	Time 0.121 (0.124)	Data 0.001 (0.003)	Loss 1.1595 (1.1672)	Prec@1 53.906 (58.308)
Epoch: [3][150/391]	Time 0.124 (0.123)	Data 0.001 (0.002)	Loss 1.1796 (1.1617)	Prec@1 58.594 (58.796)
Epoch: [3][200/391]	Time 0.124 (0.122)	Data 0.001 (0.002)	Loss 1.2670 (1.1625)	Prec@1 57.031 (58.975)
Epoch: [3][250/391]	Time 0.113 (0.122)	Data 0.001 (0.002)	Loss 1.0725 (1.1623)	Prec@1 62.500 (59.011)
Epoch: [3][300/391]	Time 0.125 (0.122)	Data 0.001 (0.002)	Loss 1.2353 (1.1576)	Prec@1 57.031 (59.118)
Epoch: [3][350/391]	Time 0.115 (0.122)	Data 0.001 (0.002)	Loss 1.2808 (1.1552)	Prec@1 54.688 (59.250)
Test: [0/79]	Time 0.154 (0.154)	Loss 1.0629 (1.0629)	Prec@1 60.938 (60.938)
Test: [50/79]	Time 0.043 (0.045)	Loss 1.1176 (1.1016)	Prec@1 57.812 (61.596)
 * Prec@1 61.620
current lr 9.99013e-03
Epoch: [4][0/391]	Time 0.262 (0.262)	Data 0.118 (0.118)	Loss 1.2184 (1.2184)	Prec@1 57.031 (57.031)
Epoch: [4][50/391]	Time 0.117 (0.124)	Data 0.001 (0.003)	Loss 1.1733 (1.1382)	Prec@1 60.938 (60.309)
Epoch: [4][100/391]	Time 0.127 (0.122)	Data 0.001 (0.002)	Loss 1.1892 (1.1265)	Prec@1 60.156 (60.566)
Epoch: [4][150/391]	Time 0.118 (0.121)	Data 0.002 (0.002)	Loss 1.1669 (1.1277)	Prec@1 61.719 (60.317)
Epoch: [4][200/391]	Time 0.118 (0.120)	Data 0.001 (0.002)	Loss 1.0690 (1.1316)	Prec@1 66.406 (60.129)
Epoch: [4][250/391]	Time 0.125 (0.120)	Data 0.001 (0.002)	Loss 1.0377 (1.1333)	Prec@1 63.281 (60.041)
Epoch: [4][300/391]	Time 0.120 (0.120)	Data 0.001 (0.002)	Loss 1.2512 (1.1364)	Prec@1 61.719 (59.972)
Epoch: [4][350/391]	Time 0.126 (0.120)	Data 0.001 (0.002)	Loss 1.0369 (1.1360)	Prec@1 63.281 (60.043)
Test: [0/79]	Time 0.155 (0.155)	Loss 1.0992 (1.0992)	Prec@1 60.156 (60.156)
Test: [50/79]	Time 0.048 (0.045)	Loss 1.2433 (1.1923)	Prec@1 56.250 (59.957)
 * Prec@1 59.750
current lr 9.98459e-03
Epoch: [5][0/391]	Time 0.264 (0.264)	Data 0.121 (0.121)	Loss 1.1778 (1.1778)	Prec@1 59.375 (59.375)
Epoch: [5][50/391]	Time 0.139 (0.123)	Data 0.001 (0.004)	Loss 1.0388 (1.1390)	Prec@1 60.938 (59.773)
Epoch: [5][100/391]	Time 0.123 (0.122)	Data 0.001 (0.002)	Loss 0.9374 (1.1359)	Prec@1 66.406 (60.373)
Epoch: [5][150/391]	Time 0.118 (0.121)	Data 0.001 (0.002)	Loss 0.8815 (1.1275)	Prec@1 67.969 (60.720)
Epoch: [5][200/391]	Time 0.120 (0.121)	Data 0.001 (0.002)	Loss 1.2624 (1.1301)	Prec@1 60.156 (60.529)
Epoch: [5][250/391]	Time 0.128 (0.122)	Data 0.002 (0.002)	Loss 1.0786 (1.1266)	Prec@1 58.594 (60.660)
Epoch: [5][300/391]	Time 0.115 (0.122)	Data 0.001 (0.002)	Loss 1.2164 (1.1254)	Prec@1 63.281 (60.808)
Epoch: [5][350/391]	Time 0.131 (0.122)	Data 0.001 (0.002)	Loss 1.3586 (1.1282)	Prec@1 54.688 (60.615)
Test: [0/79]	Time 0.181 (0.181)	Loss 1.0374 (1.0374)	Prec@1 61.719 (61.719)
Test: [50/79]	Time 0.042 (0.046)	Loss 1.2500 (1.1928)	Prec@1 55.469 (59.268)
 * Prec@1 59.460
current lr 9.97781e-03
Epoch: [6][0/391]	Time 0.269 (0.269)	Data 0.133 (0.133)	Loss 1.1482 (1.1482)	Prec@1 59.375 (59.375)
Epoch: [6][50/391]	Time 0.131 (0.128)	Data 0.002 (0.004)	Loss 1.1411 (1.1286)	Prec@1 60.156 (60.723)
Epoch: [6][100/391]	Time 0.130 (0.126)	Data 0.001 (0.003)	Loss 1.2366 (1.1430)	Prec@1 53.125 (60.249)
Epoch: [6][150/391]	Time 0.129 (0.125)	Data 0.001 (0.002)	Loss 1.1528 (1.1497)	Prec@1 64.844 (59.908)
Epoch: [6][200/391]	Time 0.129 (0.124)	Data 0.001 (0.002)	Loss 1.3138 (1.1472)	Prec@1 50.000 (59.923)
Epoch: [6][250/391]	Time 0.116 (0.124)	Data 0.001 (0.002)	Loss 1.1231 (1.1433)	Prec@1 57.031 (60.197)
Epoch: [6][300/391]	Time 0.122 (0.124)	Data 0.002 (0.002)	Loss 1.1469 (1.1495)	Prec@1 60.156 (59.995)
Epoch: [6][350/391]	Time 0.117 (0.123)	Data 0.002 (0.002)	Loss 1.1325 (1.1512)	Prec@1 58.594 (59.900)
Test: [0/79]	Time 0.153 (0.153)	Loss 1.0389 (1.0389)	Prec@1 67.969 (67.969)
Test: [50/79]	Time 0.043 (0.045)	Loss 1.2032 (1.1624)	Prec@1 53.906 (60.432)
 * Prec@1 60.730
current lr 9.96980e-03
Epoch: [7][0/391]	Time 0.264 (0.264)	Data 0.126 (0.126)	Loss 1.3707 (1.3707)	Prec@1 54.688 (54.688)
Epoch: [7][50/391]	Time 0.126 (0.124)	Data 0.001 (0.004)	Loss 1.1722 (1.1649)	Prec@1 60.156 (59.237)
Epoch: [7][100/391]	Time 0.113 (0.121)	Data 0.001 (0.003)	Loss 1.2323 (1.1891)	Prec@1 59.375 (58.826)
Epoch: [7][150/391]	Time 0.126 (0.122)	Data 0.001 (0.002)	Loss 1.2637 (1.1850)	Prec@1 60.156 (58.687)
Epoch: [7][200/391]	Time 0.124 (0.121)	Data 0.002 (0.002)	Loss 1.2202 (1.1927)	Prec@1 57.812 (58.431)
Epoch: [7][250/391]	Time 0.115 (0.121)	Data 0.001 (0.002)	Loss 1.2040 (1.2088)	Prec@1 57.812 (57.816)
Epoch: [7][300/391]	Time 0.131 (0.121)	Data 0.001 (0.002)	Loss 1.2306 (1.2238)	Prec@1 60.156 (57.254)
Epoch: [7][350/391]	Time 0.123 (0.121)	Data 0.002 (0.002)	Loss 1.5033 (1.2366)	Prec@1 50.781 (56.882)
Test: [0/79]	Time 0.160 (0.160)	Loss 1.0592 (1.0592)	Prec@1 62.500 (62.500)
Test: [50/79]	Time 0.042 (0.045)	Loss 1.1304 (1.1840)	Prec@1 58.594 (59.773)
 * Prec@1 59.340
current lr 9.96057e-03
Epoch: [8][0/391]	Time 0.278 (0.278)	Data 0.131 (0.131)	Loss 1.1707 (1.1707)	Prec@1 59.375 (59.375)
Epoch: [8][50/391]	Time 0.117 (0.123)	Data 0.002 (0.004)	Loss 1.5956 (1.3323)	Prec@1 46.094 (54.059)
Epoch: [8][100/391]	Time 0.119 (0.121)	Data 0.001 (0.003)	Loss 1.2056 (1.3071)	Prec@1 54.688 (54.834)
Epoch: [8][150/391]	Time 0.115 (0.122)	Data 0.001 (0.002)	Loss 1.3878 (1.2950)	Prec@1 56.250 (55.013)
Epoch: [8][200/391]	Time 0.117 (0.122)	Data 0.001 (0.002)	Loss 1.2574 (1.2912)	Prec@1 57.812 (55.193)
Epoch: [8][250/391]	Time 0.114 (0.122)	Data 0.001 (0.002)	Loss 1.1913 (1.2882)	Prec@1 57.812 (55.117)
Epoch: [8][300/391]	Time 0.115 (0.121)	Data 0.002 (0.002)	Loss 1.2924 (1.2919)	Prec@1 55.469 (55.066)
Epoch: [8][350/391]	Time 0.114 (0.122)	Data 0.001 (0.002)	Loss 1.5896 (1.2990)	Prec@1 45.312 (54.812)
Test: [0/79]	Time 0.168 (0.168)	Loss 1.5076 (1.5076)	Prec@1 52.344 (52.344)
Test: [50/79]	Time 0.042 (0.046)	Loss 1.5851 (1.4083)	Prec@1 41.406 (52.267)
 * Prec@1 51.920
current lr 9.95012e-03
Epoch: [9][0/391]	Time 0.300 (0.300)	Data 0.147 (0.147)	Loss 1.5492 (1.5492)	Prec@1 51.562 (51.562)
Epoch: [9][50/391]	Time 0.116 (0.130)	Data 0.001 (0.004)	Loss 1.4466 (1.5546)	Prec@1 47.656 (44.638)
Epoch: [9][100/391]	Time 0.116 (0.125)	Data 0.001 (0.003)	Loss 1.4103 (1.4998)	Prec@1 54.688 (47.030)
Epoch: [9][150/391]	Time 0.118 (0.123)	Data 0.001 (0.002)	Loss nan (nan)	Prec@1 3.125 (44.190)
Epoch: [9][200/391]	Time 0.121 (0.122)	Data 0.001 (0.002)	Loss nan (nan)	Prec@1 0.000 (33.773)
Epoch: [9][250/391]	Time 0.115 (0.121)	Data 0.001 (0.002)	Loss nan (nan)	Prec@1 11.719 (27.506)
Epoch: [9][300/391]	Time 0.132 (0.121)	Data 0.002 (0.002)	Loss nan (nan)	Prec@1 0.000 (23.502)
Epoch: [9][350/391]	Time 0.119 (0.121)	Data 0.001 (0.002)	Loss nan (nan)	Prec@1 0.000 (20.568)
Test: [0/79]	Time 0.173 (0.173)	Loss nan (nan)	Prec@1 10.156 (10.156)
Test: [50/79]	Time 0.045 (0.046)	Loss nan (nan)	Prec@1 7.812 (8.961)
 * Prec@1 8.800
current lr 9.93844e-03
Epoch: [10][0/391]	Time 0.278 (0.278)	Data 0.131 (0.131)	Loss nan (nan)	Prec@1 0.000 (0.000)
